---
title: "HW_3"
author: "Stanislav Sochynskyi, Katsiaryna Lashkevich"
date: "March 28, 2019"
output:
  html_document:
    df_print: paged
---

```{r}
library("tidyverse")
library("party")
library("randomForest")
library("rfUtilities") # for Cross-Validation
library("ROSE") # for balancing
library("pROC") # for ROC and AUC
```


The goal of this homework is to build a model which will help the company to identify the **Attrition rate** signify whether customer left (Yes) company or not (No). Let's first load the data, observe it and preprocess it before bulding model.
```{r}
dt_raw <- read.csv(file.choose())
head(dt_raw, 3)
```
```{r}
str(dt_raw)
```

Wow! An impressive number of variables, but quality matters. The data set consists of 30 different columns in total which describes employee's attributes such as age, department, job role etc. There are only two types of variables: int (21) and Factor(9). In order to use columns with numeric values for classification it is necessary to transform them to Factor type. 

The Attrition column specifies whether an employee left the company (Yes) or stay. Let's now go deeper and see the short summary of these values.

```{r}
summary(dt_raw)
```
From the summary above demostrate that columns such as *StandardHours* and *Over18* consist only from one value. Pobably, these coulmns should be erased from the dataset as they will not bring any value to the model and will slow down the speed of classification. 

```{r}
dt_raw$StandardHours <- NULL
dt_raw$Over18 <- NULL
```

#You don't have to convert all integers to factor. Only those that have a lot of different values. Numerics with 4-5-6 levels can be left as they are.

##Data preprocessing
Let's start to clean data by starting off checking the data set for missing values.
```{r}
sapply(dt_raw, function(x) sum(is.na(x)))
```
Wonderful! None of the columns contains missing data so we do not need to erase even a single row!

The next step towards building precise is to reduce the number of columns and unique values within the columns. The more columns and the more values are within the columns -> the more time it will take for an algorithm to train the model.

```{r}
func <- function(x) { #takes imput as a column
  if(class(x) == "factor") #check the class of taken column
  {
    unique(x) #print levels
  } 
  else
  {
    paste(min(x),"-", max(x)) #print range of values
  }
}
sapply(dt_raw[,-1], func)
#sapply(dt_raw, function(x) length(unique(x))) <- another way to check
```

Hurray! Now, let's start observation from data with Factor type.

Now, we have to deal with numeric columns. For columns which have a few values we will transform those columns into factors. For those columns which have higher number of unique values, we will transform them into bins.

Int columns to be directly converted to Factor type: *WorkLifeBalance*, *TrainingTimesLastYear*, *RelationshipSatisfaction*, *PerformanceRating*, *JobSatisfaction*, *JobInvolvement*, *EnvironmentSatisfaction*, *Education*. 

```{r}
dt_raw$WorkLifeBalance <- as.factor(dt_raw$WorkLifeBalance)

dt_raw$TrainingTimesLastYear <- as.factor(dt_raw$TrainingTimesLastYear)

dt_raw$RelationshipSatisfaction <- as.factor(dt_raw$RelationshipSatisfaction)

dt_raw$PerformanceRating <- as.factor(dt_raw$PerformanceRating)

dt_raw$JobSatisfaction <- as.factor(dt_raw$JobSatisfaction) # probably correlated with JobInvolvement, EnvironmentSatisfaction and RelationshipSatisfaction

dt_raw$JobInvolvement <- as.factor(dt_raw$JobInvolvement) #

dt_raw$EnvironmentSatisfaction <- as.factor(dt_raw$EnvironmentSatisfaction) #

dt_raw$Education <- as.factor(dt_raw$Education)

```

Int columns to transform splitted into bins: *YearsWithCurrManager*, *YearsSinceLastPromotion*, *YearsInCurrentRole*, *YearsAtCompany*, *TotalWorkingYears*, *NumCompaniesWorked*, *MonthlyRate*, *MonthlyIncome*, *HourlyRate*, *DistanceFromHome*, *DailyRate*.

```{r}
library('tidyverse')
#CROSS CHECK REQUIRED

dt_raw <- mutate(dt_raw, Age = cut(Age, breaks = c(17,30,36,36.92,43,60)))
#unique(dt_raw$Age)

dt_raw <- mutate(dt_raw, DailyRate = cut(DailyRate, breaks = c(101,465,802,802.5,1157,1499)))
#unique(dt_raw$DailyRate)

#summary(dt_raw$DistanceFromHome)
dt_raw <- mutate(dt_raw, DistanceFromHome = cut(DistanceFromHome, breaks = c(0,2,7,9.193,14,29)))
#unique(dt_raw$DistanceFromHome)

dt_raw <- mutate(dt_raw, HourlyRate = cut(HourlyRate, breaks = c(29,48,65.89,66,83.75,100)))
#unique(dt_raw$HourlyRate)

dt_raw <- mutate(dt_raw, MonthlyIncome = cut(MonthlyIncome, breaks = c(1008,2911,4919,6503,8379,19999)))
#unique(dt_raw$MonthlyIncome) <-SUSPICIOUS.....

#summary(dt_raw$MonthlyRate)
dt_raw <- mutate(dt_raw, MonthlyRate = cut(MonthlyRate, breaks = c(2093,8047,14236,14313,20462,26999)))
#unique(dt_raw$MonthlyRate)

#summary(dt_raw$NumCompaniesWorked)
dt_raw <- mutate(dt_raw, NumCompaniesWorked = cut(NumCompaniesWorked, breaks = c(-1,1,2,2.693,4,9)))
#unique(dt_raw$NumCompaniesWorked)

#summary(dt_raw$TotalWorkingYears)
dt_raw <- mutate(dt_raw, TotalWorkingYears = cut(TotalWorkingYears, breaks = c(-1,6,10,11.28,15,40)))
#unique(dt_raw$TotalWorkingYears)

#summary(dt_raw$YearsAtCompany )
dt_raw <- mutate(dt_raw, YearsAtCompany  = cut(YearsAtCompany , breaks = c(-1,3,5,7.008,9,40)))
#unique(dt_raw$YearsAtCompany )

#summary(dt_raw$YearsInCurrentRole)
dt_raw <- mutate(dt_raw, YearsInCurrentRole = cut(YearsInCurrentRole, breaks = c(-1,2,3,4.299,7,18)))
#unique(dt_raw$YearsInCurrentRole)

#summary(dt_raw$YearsSinceLastPromotion)
dt_raw <- mutate(dt_raw, YearsSinceLastPromotion = cut(YearsSinceLastPromotion, breaks = c(-1,0,1,2.188,3,15)))
#unique(dt_raw$YearsSinceLastPromotion)

#summary(dt_raw$YearsWithCurrManager)
dt_raw <- mutate(dt_raw, YearsWithCurrManager = cut(YearsWithCurrManager, breaks = c(-1,2,3,4.123,7,17)))
#unique(dt_raw$YearsWithCurrManager)
```
Good! Let's have small check our classes whether they are all transnsformed to Factory type.
```{r}
str(dt_raw)
```


##Task 1.

Finally, we've reached the part about training! The data is prepared let's split it.
We are going to predict whether an employee is about to leave the company. The output that we are predicting is __Attrition__. Lets create training and test data sets. We will split the dataset in
*70% - train data . 30% - test data*.
```{r}

set.seed(999) 
sample <- sample.int(n = nrow(dt_raw), size = floor(0.7*nrow(dt_raw)), replace = F)

train <- dt_raw[sample, ]
test  <- dt_raw[-sample, ]

dim(train); dim(test)
```
*1.1 First of all we apply logistic regression. For this we use glm method.* 
```{r}
LogModel <- glm(Attrition ~ ., family=binomial, data=train)
summary(LogModel)

test$predict_glm <- predict(LogModel, newdata = test[,-2], type='response') #because we predict attrition rate

ggplot(data = test, aes(x=predict_glm, fill=Attrition)) + geom_density(alpha=0.3) + theme_bw()

```
The blue color shows us that quite their job at company. The threshold is the point when the blue graph starts to overlay red one and equal to ~0.13
```{r}
test$predict_glm <- ifelse(test$predict_glm >0.13, 1, 0)
```
Now we will evaluate the model.

First, let's create empty dataframe and function to add metrics in the dataframe, which we will use to compare the models:
```{r}
report <- data.frame(description = character(), 
                     accuracy = integer(),
                     precision = integer(),
                     recall = integer(),
                     f1 = integer(),
                     stringsAsFactors=FALSE)

measures <- function(table, descr, df){
  acc <- (table[1,1] + table[2,2]) / sum(table)
  prec <- table[2,2] / (table[2,2] + table[1,2]) 
  rec <- table[2,2] / (table[2,1] + table[2,2]) 
  f1 <- 2 * prec * rec / (prec + rec)
    
  print(paste("Accuracy: ", acc))
  print(paste("Precision: ", prec))
  print(paste("Recall: ", rec))
  print(paste("F1 Score: ", f1))
  
  tmpDf <- data.frame(descr, acc, prec, rec, f1)
  colnames(tmpDf) <- colnames(df)
  df <- rbind(df, tmpDf)
  return(df)
}
```

Confusion matrix for logistic regression:
```{r}
cm_rf <- table(test$predict_glm, test$Attrition)
cm_rf
report <- measures(cm_rf, "Logistic Regression", report)
```

*Decision tree*
```{r}
tree <- ctree(Attrition ~., train)
plot(tree, type='simple')
```

The y value shows us whether the employee will leave the postition or not.
Make prediction on Test dataset
```{r}
pred_tree <- predict(tree, test)
```

Confusion matrix for logistic regression:
```{r}
cm_rf <- table(pred_tree, test$Attrition)
cm_rf
report <- measures(cm_rf, "Decision Tree", report)
```

*Random forest using proper experimental set-up (training and testing data split).*

Let's predict attrition on the test dataset with the created model 
```{r}
sapply(train, function(x) sum(is.na(x)))
train <- na.omit(train)
rfModel <- randomForest(Attrition ~., data = train)
print(rfModel)
```

Predicting attrition on the test dataset with the created model 
```{r}
pred_rf <- predict(rfModel, test)
```


Let's plot the model to check error dynamics for the different number of trees
```{r}
layout(matrix(c(1,2),nrow=1), width=c(4,1)) 
par(mar=c(5,4,4,0)) #No margin on the right side
plot(rfModel)
par(mar=c(5,0,4,2)) #No margin on the left side
plot(c(0,1), axes=F, xlab="", ylab="")
legend("top", colnames(rfModel$err.rate),col=1:4,cex=0.8,fill=1:4)

varImpPlot(rfModel, sort=T, n.var = 10, main = 'Top 10 Feature Importance')
```

As we can see the Job Role variable the number one in the list of top 10 features which is important and influential for the Random forest algorithm(value above 15). The importance of other variables such as age, OverTime, Job Satisfaction etc. is a range [10;15].

Confusion matrix for random forest:

```{r}
cm_rf <- table(pred_rf, test$Attrition)
cm_rf
report <- measures(cm_rf, "Random Forest", report)
```

```{r}
report
```


##Task 2.
Currently, employee churn is around 16%.

```{r}
dt_churn <- filter(dt_raw, Attrition=='Yes')
dt_regular <- filter(dt_raw, Attrition=='No')
dt_one_percent <- rbind.data.frame(sample_n(dt_churn, size = 15),
dt_regular)
```

```{r}
report1 <- data.frame(description = character(), 
                     accuracy = integer(),
                     precision = integer(),
                     recall = integer(),
                     f1 = integer(),
                     stringsAsFactors=FALSE)

measures <- function(table, descr, df){
  acc <- (table[1,1] + table[2,2]) / sum(table)
  prec <- table[2,2] / (table[2,2] + table[1,2]) 
  rec <- table[2,2] / (table[2,1] + table[2,2]) 
  f1 <- 2 * prec * rec / (prec + rec)
    
  print(paste("Accuracy: ", acc))
  print(paste("Precision: ", prec))
  print(paste("Recall: ", rec))
  print(paste("F1 Score: ", f1))
  
  tmpDf <- data.frame(descr, acc, prec, rec, f1)
  colnames(tmpDf) <- colnames(df)
  df <- rbind(df, tmpDf)
  return(df)
}
```

Now, using any model (random forest, logistic regression, decision tree) of your choice for classification:

*2.1 Apply this model to each of the two datasets that  is original and 1% (dt_one_percent). Calculate all the five metrics (recall, precision, F1, accuracy, AUC).*

*Original dataset*

```{r}
rfModel <- randomForest(Attrition ~., data = train)
test_pred_rf <- predict(rfModel, newdata = test)
```

Confusion matrix:
```{r}
cm_rf_original <- table(test_pred_rf, test$Attrition)
cm_rf_original
report <- measures(cm_rf_original, "Random Forest Original", report)
```



*1 % dataset*
Let's separate the data for our model training

```{r}
set.seed(999) 
sample_one_percent <- sample.int(n = nrow(dt_one_percent), size = floor(0.7*nrow(dt_one_percent)), replace = F)

train_one_percent <- dt_one_percent[sample_one_percent, ]
test_one_percent  <- dt_one_percent[-sample_one_percent, ]

dim(train_one_percent); dim(test_one_percent)
```


```{r}
rfModel_one_percent <- randomForest(Attrition ~., data = train_one_percent)
print(rfModel_one_percent)
```

```{r}
pred_rf_one_percent <- predict(rfModel_one_percent, test_one_percent)
```

```{r}
rfModel_one_percent <- randomForest(Attrition ~., data = train_one_percent)
test_pred_rf_one_percent <- predict(rfModel_one_percent, newdata = test_one_percent)
```


Confusion matrix:
```{r}
cm_rf_one_percent <- table(test_pred_rf_one_percent, test_one_percent$Attrition)
cm_rf_one_percent
report <- measures(cm_rf_one_percent, "Random Forest 1% original", report)
```



*2.2 Apply undersampling oversamplingandbothsampling techniques to improve the 1%  model. Compare  the five metrics (recall, precision, F1, accuracy, AUC) of all the three techniques with the original 1% imbalanceddataset. What are your observations?*

Let's look at values in column "Attrition"

```{r}
ggplot(dt_one_percent, aes(x=Attrition, fill= Attrition)) + geom_bar()
```

##Undersampling

```{r}
percent_undersampling <- ovun.sample(Attrition~. , data=dt_one_percent, method = "under", 
                                    p=0.5, seed = 1000)$data
```

```{r}
ggplot(percent_undersampling,aes(x=Attrition, fill= Attrition)) +
    geom_bar()
```

###Random forest

Data separation:

```{r}
sample <- sample.int(n = nrow(percent_undersampling), size = floor(0.7*nrow(percent_undersampling)), replace = F)
train_u <- percent_undersampling[sample, ]
test_u  <- percent_undersampling[-sample, ]
```

Training the model:

```{r}
rfModel_under <- randomForest(Attrition ~., data = train_u)
test_pred_rf_u <- predict(rfModel_under, newdata = test_u)
```

Confusion matrix:

```{r}
cm_rf <- table(test_pred_rf_u, test_u$Attrition)
cm_rf
report <- measures(cm_rf, "Random Forest. Undersampling", report)
```



##Oversampling

```{r}
percent_oversampling <- ovun.sample(Attrition~. , data= dt_one_percent, method = "over", 
                                    p=0.5, seed = 1000)$data
```

```{r}
ggplot(percent_oversampling,aes(x=Attrition, fill= Attrition)) +
    geom_bar()
```

###Random forest

Data separation:

```{r}
sample <- sample.int(n = nrow(percent_oversampling), size = floor(0.7*nrow(percent_oversampling)), replace = F)
train_o <- percent_oversampling[sample, ]
test_o  <- percent_oversampling[-sample, ]
```

Training the model:

```{r}
rfModel_over <- randomForest(Attrition ~., data = train_o)
test_pred_rf_o <- predict(rfModel_over, newdata = test_o)
```

Confusion matrix:

```{r}
cm_rf <- table(test_pred_rf_o, test_o$Attrition)
cm_rf
report <- measures(cm_rf, "Random Forest. Oversampling", report)
```


##Bothsampling

```{r}
percent_bothsampling <- ovun.sample(Attrition~. , data= dt_one_percent, method = "both", 
                                    p=0.5, seed = 1000)$data
```

```{r}
ggplot(percent_bothsampling,aes(x=Attrition, fill= Attrition)) +
    geom_bar()
```

###Random forest

Data separation:

```{r}
sample <- sample.int(n = nrow(percent_bothsampling), size = floor(0.7*nrow(percent_bothsampling)), replace = F)
train_b <- percent_bothsampling[sample, ]
test_b  <- percent_bothsampling[-sample, ]
```

Training the model:

```{r}
rfModel_both <- randomForest(Attrition ~., data = train_b)
test_pred_rf_b <- predict(rfModel_both, newdata = test_b)
```

Confusion matrix:

```{r}
cm_rf <- table(test_pred_rf_b, test_b$Attrition)
cm_rf
report <- measures(cm_rf, "Random Forest. Bothsampling", report)
```

```{r}
report
```

Let's calculate ROC and AUC for predicted values:

```{r}
roc <- roc(as.numeric(test$Attrition), as.numeric(test_pred_rf)) # for original data
auc(roc)

roc_one_percent <- roc(as.numeric(test_one_percent$Attrition), as.numeric(test_pred_rf_one_percent)) # for 1% original data
auc(roc_one_percent)

roc_b <- roc(as.numeric(test_b$Attrition), as.numeric(test_pred_rf_b)) # for bothsampled data
auc(roc_b) 

roc_o <- roc(as.numeric(test_o$Attrition), as.numeric(test_pred_rf_o)) # for oversampled data
auc(roc_o)

roc_u <- roc(as.numeric(test_u$Attrition), as.numeric(test_pred_rf_u)) # for undersampled data
auc(roc_u)
```

And plot all curves:

```{r}
plot.roc(roc_b) # black - bothsampled
text(0.36, 0.53, labels=sprintf("AUC(B)): %0.3f", auc(roc_b)), col="black")

lines(roc_u, col="red", type='b')
text(0.36, 0.33, labels=sprintf("AUC(U)): %0.3f", auc(roc_u)), col="red") # red - undersampled

lines(roc_o, col="green", type='b')
text(0.36, 0.43, labels=sprintf("AUC(O): %0.3f", auc(roc_o)), col="green") # green - oversampled
```




