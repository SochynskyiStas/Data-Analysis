---
title: "HW_2"
author: "Katsiaryna Lashkevich(B88167),Stanislav Sochynskyi(B88172)"
date: "March 16, 2019"
output: html_document
---
```{r}
library(dplyr)
library(ggplot2)
library(gridExtra)
library(data.table)
library(tm) # package for text mining
library(wordcloud) # for word visualization
library(ape) # dendrogram plotting
library(ggdendro) # dendrogram plotting
```

Using  orders_rfm.csv  data,  perform  the  RFM  analysis. First of all, let's remind what is RFM analysis. What is it? It means to calculate three key measures:
    R - recency score
    F - frequency score
    M - monetary score
In our case *recency* will be expressed as number of days since the last order, *frequency* will be defined as the number of purchased items, and *monetary score* is the total amount spent during the defined period.

```{r}
orders<-read.csv("orders_rfm.csv",sep=",")
head(orders,7)
```
Before moving forward let's transform order_date into date format.
```{r}
orders$order_date<-as.Date(orders$order_date, format='%Y-%m-%d')
class(orders$order_date)
```

Great! Looks like "order_date" column is transformed. Now, let's fix the reporting date. The reporting date, accroding to the Homework desciption, is “2017-04-01”(Year-Month-Date format). 
```{r}
reporting_date <- as.Date('2017-04-01', format='%Y-%m-%d')
print(reporting_date)
class(reporting_date) #additional check
```

Well done! Let's move forward. Now, we have to divide clients into bins using quantiles. Since we are interested only in orders that happened before specified date, let's filter out unneccessary orders:
```{r}
necessary_orders <- orders
necessary_orders <- filter(necessary_orders, order_date <= reporting_date)
```
We copied the initial dataset not to spoil it and keep the original data.
Also, let's add some descriptive first.
```{r}
length(unique(necessary_orders$client_id)) # Client ids
table(necessary_orders$product) # Times each product was bought
```
**At this point, the first difference is observed with a class example. The number of products a/b/c increased from 1811/788/405 up to 2355/1022/533 respectively with a change of report data.**

Let's calcualte the frequency, recency and monetary values
```{r}
frm_tbl_initial <- necessary_orders %>%
  group_by(client_id) %>%
  summarise(order_frequency = n(), # amount of products
            order_recency = min(reporting_date - order_date), 
            # days since last order 
            order_monetary = sum(money_spent)) # total amount spent
head(frm_tbl_initial)
```
As we know, order recency is a time object (days), which can cause errors later. We need to transform it into numeric value:
```{r}
frm_tbl_initial$order_recency <- as.numeric(frm_tbl_initial$order_recency) 
class(frm_tbl_initial$order_recency)
```

Wow! We've done so much for so little time! We read the data, transformed it, filtered based on the data and counted RFM. Now we can start dividing clients into bins using quantiles. 
Let's see the limits first.
```{r}
summary(frm_tbl_initial) # to explore limits
```
Let's set a breaks accroding to the data above.
```{r}
fr_tbl <- mutate(frm_tbl_initial, 
                 frequency_bins = cut(order_frequency,
                                      breaks = c(0,5,8,9,12,32)))
table(fr_tbl$frequency_bins)

```
Comparing to class results, the variation of order frequency has not significantly changed. 
```{r}
fr_tbl <- mutate(fr_tbl, 
                 recency_bins = cut(order_recency,
                                      breaks = c(-1,12,30,34,51,89)))
table(fr_tbl$recency_bins)
```
Comparing to class results, the maximum amount of days since the last order changed significantly changed up to 89 days. 
```{r}
fr_tbl <- mutate(fr_tbl, 
                 monetary_bins = cut(order_monetary, 
                                     breaks=c(5,414,761,880,1215,3495)))
table(fr_tbl$monetary_bins)
```

Comparing to class results, the variation of monetary score didn't change. The value of all quantiles changed. The minimum value decreased to 5.48. The maximum value increased from 3345 up to 3493.15, the 3rd Qu. has value 1215.03 comparing to 1008 that we had in a class etc.

Now, we have the clients allocated into bins, which provides already a good intution, what kind of customers and how many of those we have. 

```{r}
fr_tbl$frequency_bins <- factor(fr_tbl$frequency_bins, levels=rev(levels(fr_tbl$frequency_bins)))
fr_tbl$monetary_bins <- factor(fr_tbl$monetary_bins, levels=rev(levels(fr_tbl$monetary_bins)))

table(frequency=fr_tbl$frequency_bins, recency=fr_tbl$recency_bins, monetary=fr_tbl$monetary_bins)
```

Let's build the heatmap:
```{r}

fr_tbl_counts <- fr_tbl %>% group_by(frequency_bins, recency_bins) %>% summarise(count=n())
p_basic <- ggplot(fr_tbl_counts, aes(x=recency_bins, y=frequency_bins)) + geom_tile(aes(fill=count))+ 
  geom_text(aes(label = count)) +
  scale_fill_gradient(low='#d4f4ad', high="#f24236") + theme_bw(base_size=20)
p_basic
```
Let's devide the heatmap into *5 segments*: _New_, _Lost_, _Promising_, _Loyal customers_, _Hibernating loyal customers_.
```{r}

p_quadrants <- p_basic +
  ggplot2::annotate("rect", xmin = 0, xmax=3.47, ymin=3.47, ymax=6, color='green', alpha=0.1, fill='green') +
  ggplot2::annotate("rect", xmin = 0, xmax=3.47, ymin=2.5, ymax=3.47, color='yellow', alpha=0.1, fill='yellow') +
  ggplot2::annotate("rect", xmin = 0, xmax=3.47, ymin=0, ymax=2.5, color='blue', alpha=0.1, fill='blue') +
  ggplot2::annotate("rect", xmin = 3.5, xmax=6, ymin=3.47, ymax=6, color='red', alpha=0.1, fill='red') +
  ggplot2::annotate("rect", xmin = 3.5, xmax=6, ymin=0, ymax=3.47, color='black', alpha=0.1, fill='black')
p_quadrants +
  ggplot2::annotate("text", x=1.8, y=5.8, label='New') +
  ggplot2::annotate("text", x=4.8, y=5.8, label='Lost') +
  ggplot2::annotate("text", x=1.4, y=2.7, label='Promising') +
  ggplot2::annotate("text", x=1.8, y=0.2, label='Loyal customers') +
  ggplot2::annotate("text", x=4.8, y=0.2, label='Hibernating loyal customers')
```

**Answer**: In general, the number of customers who stopped to buy new items is higher than the total number of customers both segments of _New_ and _Promising_ customers together - 121 vs. (103+17) respectively. The number of _Loyal customers_ slightly decreased from 40 to 35 comparing to the class results. _Promising segment_ as well as _Hibernating loyal customer segment_ almost stayed at the same level comparing to the class data. Based on the information mentioned above the further analysis must be conducted to identify the source of customers churns.


Imagine  that  you  are  working  in a company  that  offers  different types  of furniture. Use the customer survey _data customer_survey.csv_ that contains information on customers. Columns:
```{r}
survey<-read.csv("customer_survey.csv",sep=" ")
head(survey,7)
```
age - age of the customer;\
gender - male or female;\
income - total income of the customer;\
kids - number of kids;\
ownHome - whether customer owns house or not;\
subscribe -  whether customer subscribed to mailing service or not;\

Execute the following tasks:

2.1 a) Convert   all   factor   features   to   numerical.   Each   different   value   should   be converted to separate number. Example: Yes => 1; No => 0.

```{r}
#levels(survey$gender)
survey$gender<-as.numeric(survey$gender) - 1
survey$ownHome<-as.numeric(survey$ownHome) - 1
survey$subscribe<-as.numeric(survey$subscribe) - 1

head(survey,10)

```
2.1 b) As a next step, rescale all features. We need to scale features by substructing mean and dividing by standard deviation. This can be done using scale function. 
```{r}
data_clustering <- survey %>%
  mutate(age=scale(age), 
         gender=scale(gender), 
         income=scale(income),
         kids=scale(kids),
         ownHome=scale(ownHome),
         subscribe=scale(subscribe))
head(data_clustering)
```
2.2 Use  Elbow  method  to  determinate  optimal  amount  of clusters for  k-means  clustering. For  each amount  of clusters you  take  use  at  least 10  different initial centers. Report the most optimal number of clusters. 

Usually there is a tradeoff, as it is required to have as few clusters as possible, because it is easy to interpret them (nobody wants to interpret and act upon 79 segments of customers), but we want to minimize the _wss_ statistics. The dea behind the eilbow methods is that we want to find a point, where the wss is small, but the function does not become smooth (adding more clusters does not reduce much of the variance), so we are looking for an angle in the curve.
```{r}
# finding optimal number of clusters
elbow_method <- function(data, max_k=100){
  require(ggplot2)
  wss <- 0
  for (i in 1:max_k){
    wss[i] <- sum(kmeans(data, centers=i)$withinss)
  }
  p <- data.frame(number_of_clusters=c(1:max_k), wss=wss) %>%
    ggplot(aes(x=number_of_clusters, y=wss)) + geom_point() + 
    geom_line() + theme_bw() + ylab("Within groups sum of squares") #QUESTION
  return(print(p))
}

# apply the function
elbow_method(data_clustering[,-1],max_k=25)
```
The graph above shows that the optimal number of clusters is 9. It is also impossible to go further for 11-13 cluster, however, it really depends on the ability of an analyst to the to interpret and explain resulted clusters. Let's vizualize clusters based on the ralations of age and income of the customers.
```{r}
clusters <- kmeans(data_clustering[,-1], centers = 9, nstart=20)
clusters

data_clustering$cluster <- as.factor(clusters$cluster) # we need it to be a factor for the plot
ggplot(data_clustering, aes(x=age, y=income, color=cluster)) + geom_point(size=2)
```
In the graph above it is hard to identify the pattern and interpred the received results thus the number of clusters should be decreased. However, it is not the goal of this homework. 


2.3 The second clustering method that we can use for segmentation is hierarchical clustering. Let's pick 30 records randomly from the dataset and perform hierarchical clustering. For this we need to set a seed. Let it be 42.
```{r}
set.seed(42)
random_data_set<- sample_n(data_clustering, 30)
random_data_set<- as.data.frame(random_data_set) # we need this function just because the dendrogram cannot see labels

distance_m <- dist(as.matrix(random_data_set[,-1])) 
hc <- hclust(distance_m)
plot(hc)
```